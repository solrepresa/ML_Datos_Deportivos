{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM_Deporte.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuRjscfXRpR9rdzNujLGzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solrepresa/ML_Datos_Deportivos/blob/master/LM_Deporte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nfg1WIEMn1Y",
        "colab_type": "text"
      },
      "source": [
        "# Análisis de Datos Deportivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbe2pspEMw_w",
        "colab_type": "text"
      },
      "source": [
        "## 0) Configurar el entorno\n",
        "\n",
        "**¡Importantísimo!** \n",
        "\n",
        "Instalar JAVA en Google Colab. El siguiente código instala Apache Spark 2.4.4, Java 8 y Findspark , una biblioteca que facilita a Python encontrar Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQpXbha3Mjz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Gn-iL9M5F7",
        "colab_type": "code",
        "outputId": "5a84948f-6d98-4779-e5cd-7d28ce3b046b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "! java -version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.6\" 2020-01-14\n",
            "OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sepQ3PUNM9la",
        "colab_type": "code",
        "outputId": "4726e856-1437-49ca-8b74-6a84b939fcdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Cargamos librerias\n",
        "!pip install --upgrade pyspark\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "#from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.session import SparkSession\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=35caaac7b0bbd279116053c98b9485ff48581be1118f9dc37f4cc45e75c0c3e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IagAjg9NKQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6icVeVooM4FD",
        "colab_type": "text"
      },
      "source": [
        "Creamos un contexto y sesión para Spark. Luego lo vamos a cerrar al final del código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In5FiJmANSYJ",
        "colab_type": "code",
        "outputId": "3e482255-eecb-4a30-86cb-a19d88e621c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Configuración de entorno\n",
        "confspark = SparkConf()\n",
        "confspark.set(\"spark.master\", \"local\") # 2 hilos ¿como se dice en español?\n",
        "confspark.set(\"spark.app.name\", \"AppPruebaMBDD\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7fa043aa30b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ku9lInMNn9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inicialización\n",
        "sc = SparkContext(conf = confspark)\n",
        "sqlcx = HiveContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35U9wF-DTaPP",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------------------------------------\n",
        "## 1) Preparación y descripción de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydh3rqv_Oor-",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Cargar datos\n",
        "\n",
        "Para la carga de datos utilizamos Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpcI05u3OoYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abrir conjunto de datos\n",
        "url = \"http://www.football-data.co.uk/mmz4281/1718/SP1.csv\"\n",
        "df = pd.read_csv(url)\n",
        "data = sqlcx.createDataFrame(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbYXm1-GPQWP",
        "colab_type": "code",
        "outputId": "0505295d-7d03-45ae-dc23-e07fa137213b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta_4TRFoO7ud",
        "colab_type": "code",
        "outputId": "244568c1-aa6b-4b6e-cc70-6518bc21d3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "data.show(10) # Para visualizar las primeras 10 filas\n",
        "#data.printSchema() # Nos muestra en detalle las clases de nuestras variables"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|   HomeTeam|   AwayTeam|FTHG|FTAG|FTR|HTHG|HTAG|HTR| HS| AS|HST|AST| HF| AF| HC| AC| HY| AY| HR| AR|\n",
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|    Espanol|      Betis|   1|   0|  H|   0|   0|  D| 10|  9|  4|  4| 10| 17|  5|  3|  2|  5|  0|  0|\n",
            "|    Leganes|     Malaga|   2|   0|  H|   0|   0|  D| 10|  4|  3|  1| 11| 14|  6|  1|  1|  1|  0|  0|\n",
            "|    Sevilla|Real Madrid|   3|   2|  H|   2|   0|  H| 14| 14|  6|  4| 21|  9|  2|  3|  3|  1|  0|  0|\n",
            "|  Barcelona|   Sociedad|   1|   0|  H|   0|   0|  D| 12| 13|  4|  2| 13| 13|  3|  8|  3|  2|  0|  0|\n",
            "|Real Madrid|      Betis|   0|   1|  A|   0|   0|  D| 27| 12|  7|  4| 13| 11| 12|  3|  0|  3|  0|  0|\n",
            "|     Girona|     Alaves|   2|   3|  A|   0|   0|  D| 11|  9|  3|  4|  8| 13|  6|  5|  1|  2|  0|  0|\n",
            "|  La Coruna|      Betis|   0|   1|  A|   0|   0|  D| 16| 10|  2|  4| 19|  7|  8|  1|  2|  0|  0|  0|\n",
            "|    Leganes|   Sociedad|   1|   0|  H|   0|   0|  D|  8|  9|  4|  5| 14| 11|  5|  6|  0|  1|  0|  0|\n",
            "|    Levante|    Sevilla|   2|   1|  H|   1|   1|  D| 15| 22|  5|  4|  4| 15|  4|  8|  1|  1|  0|  0|\n",
            "|     Malaga|      Eibar|   0|   1|  A|   0|   0|  D| 10| 13|  4|  6| 16| 15|  3|  7|  2|  3|  0|  0|\n",
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta7_yHbGUlKE",
        "colab_type": "text"
      },
      "source": [
        "Seleccionamos las variables para trabajar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBNyq0caaco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data.columns[2:22]\n",
        "data = data.select(data.columns[2:22]) # Nos quedamos con las columnas q nos interesan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGKpNQAhO-lU",
        "colab_type": "text"
      },
      "source": [
        "Información sobre los datos:\n",
        "\n",
        "* *HomeTeam* es el equipo local. *AwayTeam* es el equipo visitante.\n",
        "\n",
        "* *HY*, *HR*, *AY*, *AR* son el número de tarjetas amarillas y rojas para los equipos locales y visitantes en cada partido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbkfO3BNbaNn",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Limpieza de NAN y NULL\n",
        "\n",
        "\n",
        "Limpieza de NAN y NULL: ¿Hay NAN o null?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJBb07mWbbt6",
        "colab_type": "code",
        "outputId": "4efebaba-834d-49e6-ef9d-848bad26a9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "data.select([F.count(F.when(F.isnan(c),c)).alias(c) for c in data.columns]).show() #Numero de NAN\n",
        "data.select([F.count(F.when(F.isnull(c),c)).alias(c) for c in data.columns]).show() #Numero de Null"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|HomeTeam|AwayTeam|FTHG|FTAG|FTR|HTHG|HTAG|HTR| HS| AS|HST|AST| HF| AF| HC| AC| HY| AY| HR| AR|\n",
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|       0|       0|   0|   0|  0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "\n",
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|HomeTeam|AwayTeam|FTHG|FTAG|FTR|HTHG|HTAG|HTR| HS| AS|HST|AST| HF| AF| HC| AC| HY| AY| HR| AR|\n",
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|       0|       0|   0|   0|  0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "+--------+--------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jLj4H4kQNip",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Parámetros descriptivos\n",
        "Previo a obtener la descripción estadística de los datos eliminamos las filas repetidas, esto lo hacemos con la función **distinct()**. Los parámetros estadísticos los podemos ver con la función **describe()**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgkWk0O8W7ti",
        "colab_type": "code",
        "outputId": "086c32fd-8491-4a74-95b6-1b300f53302e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "data = data.distinct() # Por si hay datos duplicados\n",
        "data.describe().show() # Para ver estadísticos descriptivos"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+------------------+------------------+----+------------------+------------------+----+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+\n",
            "|summary|  HomeTeam|  AwayTeam|              FTHG|              FTAG| FTR|              HTHG|              HTAG| HTR|                HS|                AS|               HST|              AST|               HF|                AF|               HC|                AC|                HY|                AY|                 HR|                 AR|\n",
            "+-------+----------+----------+------------------+------------------+----+------------------+------------------+----+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+\n",
            "|  count|       380|       380|               380|               380| 380|               380|               380| 380|               380|               380|               380|              380|              380|               380|              380|               380|               380|               380|                380|                380|\n",
            "|   mean|      null|      null|1.5473684210526315|1.1473684210526316|null|0.6605263157894737|0.4868421052631579|null|13.526315789473685|10.471052631578948| 4.757894736842105|3.805263157894737|13.73421052631579|13.952631578947368|5.613157894736842| 4.192105263157894|2.3394736842105264|2.6763157894736844|0.11052631578947368|0.07894736842105263|\n",
            "| stddev|      null|      null|1.3784502104324623|1.1867200184400557|null|0.8521397787170123|0.6676351572795292|null|4.5379481264297095| 4.215401331750506|2.4461084897256384|2.316391900029653|4.301976984061267|    4.341353997786|2.676061282100535|2.4650465835507305|1.5792251989051789|1.5991313952502777|0.32225252203823873|0.27961327867808655|\n",
            "|    min|    Alaves|    Alaves|                 0|                 0|   A|                 0|                 0|   A|                 2|                 1|                 0|                0|                4|                 0|                0|                 0|                 0|                 0|                  0|                  0|\n",
            "|    max|Villarreal|Villarreal|                 7|                 6|   H|                 5|                 3|   H|                30|                24|                14|               13|               29|                29|               16|                14|                 8|                 9|                  2|                  2|\n",
            "+-------+----------+----------+------------------+------------------+----+------------------+------------------+----+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrUPwW9pdCgG",
        "colab_type": "text"
      },
      "source": [
        "Se propone como actividad transformar variables numéricas a categóricas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PkrwlOdKcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Faltas del equipo local\n",
        "data_transf = data.withColumn(\"CategoriaFaltas\", F.when(F.col(\"HF\").between(10,20), \"MEDIA\").when(F.col(\"HF\")<10, \"POCAS\").otherwise(\"MUCHAS\"))\n",
        "\n",
        "# Total de tarjetas en el partido\n",
        "data_transf = data_transf.withColumn(\"TotalTarjeta\", data.HY + data.HR + data.AY + data.AR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e54DPgJGfLrE",
        "colab_type": "code",
        "outputId": "9adeba09-41ac-49d5-a70c-1b2317a75bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "data_transf.show(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---------------+------------+\n",
            "|   HomeTeam|   AwayTeam|FTHG|FTAG|FTR|HTHG|HTAG|HTR| HS| AS|HST|AST| HF| AF| HC| AC| HY| AY| HR| AR|CategoriaFaltas|TotalTarjeta|\n",
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---------------+------------+\n",
            "|    Espanol|      Betis|   1|   0|  H|   0|   0|  D| 10|  9|  4|  4| 10| 17|  5|  3|  2|  5|  0|  0|          MEDIA|           7|\n",
            "|    Leganes|     Malaga|   2|   0|  H|   0|   0|  D| 10|  4|  3|  1| 11| 14|  6|  1|  1|  1|  0|  0|          MEDIA|           2|\n",
            "|    Sevilla|Real Madrid|   3|   2|  H|   2|   0|  H| 14| 14|  6|  4| 21|  9|  2|  3|  3|  1|  0|  0|         MUCHAS|           4|\n",
            "|  Barcelona|   Sociedad|   1|   0|  H|   0|   0|  D| 12| 13|  4|  2| 13| 13|  3|  8|  3|  2|  0|  0|          MEDIA|           5|\n",
            "|Real Madrid|      Betis|   0|   1|  A|   0|   0|  D| 27| 12|  7|  4| 13| 11| 12|  3|  0|  3|  0|  0|          MEDIA|           3|\n",
            "+-----------+-----------+----+----+---+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xWX_dg1kgH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalizacion de las variables (no es necesario porq los métodos son no parametricos)\n",
        "\n",
        "#from pyspark.sql.function import udf\n",
        "#from pyspark.sql.types import FloatType\n",
        "\n",
        "## Definimos una funcion para normalizar los valores\n",
        "#def normalize(valor, media, desviacion):\n",
        "#  return(valor - media)/desviacion\n",
        "\n",
        "## Registramos la funcion como funcion de usuario\n",
        "# normalize_udf = udf(normalize, FloatTipe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzQng9qcmi27",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Convertir datos *string* \n",
        "Antes de aplicar el modelo necesitamos convertir las variables de tipo texto. ¿Cuáles son las variables tipo string que tenemos?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlaez8DOmvJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be3a3606-7ecd-4170-f7ce-aaa9bc22e877"
      },
      "source": [
        "# Seleccionamos columnas de tipo string\n",
        "catcolums = [item[0] for item in data_transf.dtypes if item[1].startswith('string')] \n",
        "print(catcolums)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'CategoriaFaltas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHFf9LZLmpd8",
        "colab_type": "text"
      },
      "source": [
        "Convertir las variables texto en categóricas llevan varios pasos:\n",
        "\n",
        "1.   Definimos las variables de ingreso al modelo con **StringIndexer()**\n",
        "2.   Entrenamos el modelo con **fit**\n",
        "3.   Transformamos nuestros datos con **transform**\n",
        "\n",
        "\n",
        "La función **StringIndexer()** convierte una columna *string* en una columna *index* que es tratada con una variable categorica por spark. Los indices comienzan en 0 y avanzan por la frecuencia con la que aparecen.\n",
        "\n",
        "Luego, con la función **OneHotEncoder()** convertimos la variable del tipo categórico en dummy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDFTirjDqBn",
        "colab_type": "text"
      },
      "source": [
        "Veamos una forma rápida de hacer todo esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISmZkXFbCETX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "all_stages = [StringIndexer(inputCol=c, outputCol='idx_' + c) for c in catcolums] + [OneHotEncoder(inputCol='idx_' + c, outputCol='ohe_' + c) for c in catcolums]\n",
        "\n",
        "data_new = Pipeline(stages = all_stages).fit(data_transf).transform(data_transf) #de lista a dataFrame\n",
        "\n",
        "#data_new.show(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBoY3pxYE-w2",
        "colab_type": "text"
      },
      "source": [
        "Ya teniendo todos los datos como números, procedemos a armar el FEATURES.\n",
        "\n",
        "\n",
        "Para crear la columna FEATURES en utilizamos la función **VectorAssembler()**. Primero, creamos el modelo que nos permite transformar nuestros datos de entrada en una única columna y luego transformamos nuestros datos con la función **transform()**. Con la función **drop()** quitamos las columnas que no nos van a servir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibvsbThbowmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = VectorAssembler(inputCols=['ohe_HomeTeam', 'ohe_AwayTeam', 'ohe_FTR',  'ohe_HTR', 'ohe_CategoriaFaltas',\n",
        "                                       'FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC',\n",
        "                                       'AC', 'HY', 'AY', 'HR', 'AR'], \n",
        "                            outputCol='featuresCol').transform(data_new).drop('HomeTeam', 'HTR','AwayTeam', 'FTR','CategoriaFaltas',\n",
        "                                                                              'idx_HomeTeam', 'idx_AwayTeam', 'idx_FTR', 'idx_HTR',\n",
        "                                                                              'idx_CategoriaFaltas')\n",
        "\n",
        "#output.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuQyR4mwKZWM",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 Crear set de entrenamiento y de prueba\n",
        "\n",
        "Antes de continuar, dividimos nuestros datos en un set de entrenamiento (80%) y otro de prueba (20%):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqBaJsLaKV_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainingData, testData) = output.randomSplit([0.8, 0.2], seed = 132)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq9yon1ITInw",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "\n",
        "## 2) Implementación y análisis de algún modelo de clasificación\n",
        "\n",
        "### 2.1) Modelo de **Regresión logística multinomial**\n",
        "Utilizamos la función **LogisticRegression()** para crear el modelo. El parámetro *maxIter* determina el número de iteraciones. Luego con **fit()** lo entrenamos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv2UB-qeTsX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Definimos el modelo > Regresión logística multinomial\n",
        "lr = LogisticRegression(featuresCol = 'featuresCol', labelCol = 'TotalTarjeta', maxIter=10).fit(trainingData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dx7zDitQaIH",
        "colab_type": "text"
      },
      "source": [
        "Para obtener los parámetros del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgvIdjt3MOOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Parámetros del modelo de Regresión Logística Multinomial\n",
        "# print(\"Coeficientes: \\n\" + str(lr.coefficientMatrix))\n",
        "# print(\"Intercepcion: \" + str(lr.interceptVector))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6UxL1wQc9Y",
        "colab_type": "text"
      },
      "source": [
        "Más información del modelo se obteniene con la función **summary()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19GlB1qxP4J6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingSummary = lr.summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-XoBAW1MiSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "201e3640-9da5-4d21-adb7-60fe65ff98e3"
      },
      "source": [
        "# Obtain the objective per iteration\n",
        "objectiveHistory = trainingSummary.objectiveHistory\n",
        "print(\"objectiveHistory:\")\n",
        "for objective in objectiveHistory:\n",
        "    print(objective)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "objectiveHistory:\n",
            "2.3011589395526375\n",
            "1.9477005983734137\n",
            "1.642180025829503\n",
            "1.5235086179411625\n",
            "1.4415715996926406\n",
            "1.3686393703528563\n",
            "1.2630686208374537\n",
            "1.1575768880135986\n",
            "1.064836998162405\n",
            "0.971746981455381\n",
            "0.9342153048003732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjwxpcFkMp9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for multiclass, we can inspect metrics on a per-label basis\n",
        "print(\"False positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"True positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"Precision by label:\")\n",
        "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
        "    print(\"label %d: %s\" % (i, prec))\n",
        "\n",
        "print(\"Recall by label:\")\n",
        "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
        "    print(\"label %d: %s\" % (i, rec))\n",
        "\n",
        "print(\"F-measure by label:\")\n",
        "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
        "    print(\"label %d: %s\" % (i, f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFxemim-QnXt",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Evaluamos el ajuste del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfz79VcOThik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = trainingSummary.accuracy\n",
        "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
        "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
        "fMeasure = trainingSummary.weightedFMeasure()\n",
        "precision = trainingSummary.weightedPrecision\n",
        "recall = trainingSummary.weightedRecall\n",
        "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
        "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPdeyIYxNtXZ",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------------------------------\n",
        "## 3 - Implementación y análisis de algún modelo de regresión\n",
        "\n",
        "### 3.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjq9iJJdHdik",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------------------------------------\n",
        "## Webs consultadas\n",
        "¡Gracias a la comunidad que construye conocimiento y lo pone a disposición!\n",
        "\n",
        "* Este sitio me resultó de muchísima utilidad: https://mingchen0919.github.io/learning-apache-spark/ "
      ]
    }
  ]
}